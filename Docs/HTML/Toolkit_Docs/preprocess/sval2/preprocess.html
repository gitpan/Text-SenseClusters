<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>preprocess.pl</title>
<link rev="made" href="mailto:jluttine@marimba.d.umn.edu" />
</head>

<body style="background-color: white">

<p><a name="__index__"></a></p>
<!-- INDEX BEGIN -->

<ul>

	<li><a href="#name">NAME</a></li>
	<li><a href="#synopsis">SYNOPSIS</a></li>
	<li><a href="#usage">USAGE</a></li>
	<li><a href="#input">INPUT</a></li>
	<ul>

		<li><a href="#required_arguments_">Required Arguments:</a></li>
		<ul>

			<li><a href="#source">SOURCE</a></li>
		</ul>

		<li><a href="#optional_arguments">Optional Arguments</a></li>
		<ul>

			<ul>

				<li><a href="#token_file">-token FILE</a></li>
				<li><a href="#removenottoken">-removeNotToken</a></li>
				<li><a href="#nontoken_file">-nontoken FILE</a></li>
				<li><a href="#noxml">-noxml</a></li>
				<li><a href="#xml_file">-xml FILE</a></li>
				<li><a href="#nocount">-nocount</a></li>
				<li><a href="#count_file">-count FILE</a></li>
				<li><a href="#uselexelt">-useLexelt</a></li>
				<li><a href="#usesenseid">-useSenseid</a></li>
				<li><a href="#split_n">-split N</a></li>
				<li><a href="#seed_n">-seed N</a></li>
				<li><a href="#putsentencetags">-putSentenceTags</a></li>
				<li><a href="#version">-version</a></li>
				<li><a href="#help">-help</a></li>
				<li><a href="#verbose">-verbose</a></li>
			</ul>

		</ul>

		<li><a href="#output">OUTPUT</a></li>
	</ul>

	<li><a href="#an_example_senseval2_file">An Example SENSEVAL-2 File</a></li>
	<li><a href="#detailed_description">Detailed Description</a></li>
	<ul>

		<li><a href="#tokenization_of_text">Tokenization of Text</a></li>
		<li><a href="#various_issues_of_tokenization_wrt_preprocess_pl">Various Issues of Tokenization wrt preprocess.pl</a></li>
		<li><a href="#default_regular_expressions_">Default Regular Expressions:</a></li>
		<li><a href="#regular_expression___s___">Regular Expression /\S+/:</a></li>
		<li><a href="#regular_expression___w___">Regular Expression /\w+/:</a></li>
		<li><a href="#other_useful_regular_expressions_in_the_token_file_">Other Useful Regular Expressions in the Token File:</a></li>
		<li><a href="#order_of_regular_expressions_is_important_">Order of Regular Expressions Is Important:</a></li>
		<li><a href="#redundant_regular_expressions_">Redundant Regular Expressions:</a></li>
		<li><a href="#ignoring_nontokens_using_removenottoken_">Ignoring Non-Tokens using -removeNotToken:</a></li>
		<li><a href="#ignoring_nontokens_using_nontoken_">Ignoring Non-Tokens using -nontoken:</a></li>
		<li><a href="#xml_output_">XML output:</a></li>
		<li><a href="#count_output_">Count output:</a></li>
		<li><a href="#information_insertion">Information Insertion</a></li>
		<ul>

			<li><a href="#inserting_lexelt_and_senseid_information_">Inserting lexelt and senseId Information:</a></li>
			<li><a href="#inserting_sentenceboundary_tags">Inserting Sentence-Boundary Tags</a></li>
		</ul>

		<li><a href="#splitting_input_lexical_files">Splitting Input Lexical Files</a></li>
	</ul>

	<li><a href="#author">AUTHOR</a></li>
	<li><a href="#copyright">COPYRIGHT</a></li>
</ul>
<!-- INDEX END -->

<hr />
<p>
</p>
<h1><a name="name">NAME</a></h1>
<p>preprocess.pl</p>
<p>
</p>
<hr />
<h1><a name="synopsis">SYNOPSIS</a></h1>
<p>Takes an xml file in SENSEVAL-2 lexical-sample format
and splits it apart into as many files as there are
lexical elements in the original file. Each lexical 
element usually corresponds with a word used in a 
particular part of speech. It also does other sundry 
preprocessing tasks with the data.</p>
<p>
</p>
<hr />
<h1><a name="usage">USAGE</a></h1>
<p>preprocess.pl [OPTIONS] SOURCE</p>
<p>
</p>
<hr />
<h1><a name="input">INPUT</a></h1>
<p>
</p>
<h2><a name="required_arguments_">Required Arguments:</a></h2>
<p>
</p>
<h3><a name="source">SOURCE</a></h3>
<p>Senseval2 formatted input file.</p>
<p>
</p>
<h2><a name="optional_arguments">Optional Arguments</a></h2>
<p>
</p>
<h4><a name="token_file">--token FILE</a></h4>
<p>Reads tokens from FILE. The context of each instance
is broken up into tokens and each pair of consecutive tokens
are separated by white space. Non-white space characters
which do not belong to any token are put between angular
brackets. If this option is not used, the default token
definitions of count.pl are assumed.</p>
<p>
</p>
<h4><a name="removenottoken">--removeNotToken</a></h4>
<p>Removes strings that do not match token file. If not\n``;    
specified, the non-matching strings are put within angular\n'';
brackets, ie, &lt;&gt;</p>
<p>
</p>
<h4><a name="nontoken_file">--nontoken FILE</a></h4>
<p>Removes all characters sequences that match Perl
regular expressions specified in FILE.</p>
<p>
</p>
<h4><a name="noxml">--noxml</a></h4>
<p>Does not output an xml file.</p>
<p>
</p>
<h4><a name="xml_file">--xml FILE</a></h4>
<p>Outputs the changed xml file to FILE. If this option nor
the option --noxml is provided, the file name is derived
by concatenating the word in the &lt;lexelt&gt; tag with \``.xml\''.
Note: if this option is used, separate lexelt items will
not be split into separate files.</p>
<p>
</p>
<h4><a name="nocount">--nocount</a></h4>
<p>Does not output a NSP-ready file.</p>
<p>
</p>
<h4><a name="count_file">--count FILE</a></h4>
<p>Outputs just the part between &lt;context&gt; &lt;/context&gt; (after
modification) to FILE. FILE can then be used directly with
NSP. If this option nor the option --nocount is provided,
the file name is derived as in xml above, with a .count
extension.
Note: if this option is used, separate lexelt items will
not be split into separate files.</p>
<p>
</p>
<h4><a name="uselexelt">--useLexelt</a></h4>
<p>Includes a tag &lt;lexelt=WORD/&gt; within the &lt;head&gt;&lt;/head&gt;
tags, where WORD is the word in the immediately preceeding
&lt;lexelt&gt; tag.</p>
<p>
</p>
<h4><a name="usesenseid">--useSenseid</a></h4>
<p>Includes a tag &lt;senseid=XXXXX/&gt; within the &lt;head&gt;&lt;/head&gt;
tags, where XXXXX is the number in the immediately
preceeding &lt;answer&gt; tag.</p>
<p>
</p>
<h4><a name="split_n">--split N</a></h4>
<p>Shuffles the instances in SOURCE and then splits them into
two files, a training file and a test file, approximately
in the ratio N:(100-N).</p>
<p>
</p>
<h4><a name="seed_n">--seed N</a></h4>
<p>Sets the seed for the random number generator used during
shuffling. If not used, no seeding is done (except for
that provided automatically by perl)</p>
<p>
</p>
<h4><a name="putsentencetags">--putSentenceTags</a></h4>
<p>Puts separate lines within the &lt;context&gt; &lt;/context&gt; region
within &lt;s&gt; &lt;/s&gt; pairs of tags. If separate sentences are on
seperate lines, these tags effectively denote the start and
end of sentences.</p>
<p>
</p>
<h4><a name="version">--version</a></h4>
<p>Prints the version number.</p>
<p>
</p>
<h4><a name="help">--help</a></h4>
<p>Prints this help message.</p>
<p>
</p>
<h4><a name="verbose">--verbose</a></h4>
<p>Turns on verbose mode. Silent by default.</p>
<p>
</p>
<h2><a name="output">OUTPUT</a></h2>
<p>1. The modified/processed Input SENSEVAL-2 (*.xml) file, if --noxml option is not specified.</p>
<p>2. The Ngram Statistics Package (NSP) ready (*.count) file, if --nocount is not specified.</p>
<p>3. The *-test and *-training files if the --split option is used.</p>
<p>
</p>
<hr />
<h1><a name="an_example_senseval2_file">An Example SENSEVAL-2 File</a></h1>
<p>The following is an example SENSEVAL-2 file that we will refer to
later in as example.xml</p>
<p>&lt;corpus lang='english'&gt;
  &lt;lexelt item=``art.n''&gt;
    &lt;instance id=``art.40001''&gt;
      &lt;answer instance=``art.40001'' senseid=``art~1:06:00::''/&gt;
      &lt;answer instance=``art.40001'' senseid=``fine_art%1:06:00::''/&gt;
      &lt;context&gt;
        &lt;head&gt;Art&lt;/head&gt; you can dance to from the creative group
        called Halo.
      &lt;/context&gt;
    &lt;/instance&gt;
    &lt;instance id=``art.40002''&gt;
      &lt;answer instance=``art.40002'' senseid=``art_gallery~1:06:00::''/&gt;
      &lt;context&gt;
        There's always one to be heard somewhere during the summer in
        the piazza in front of the &lt;head&gt;art&lt;/head&gt;gallery and town
        hall or in a park.
      &lt;/context&gt;
    &lt;/instance&gt;
    &lt;instance id=``art.40005'' docsrc=``bnc_ckv_938''&gt;
    &lt;answer instance=``art.40005'' senseid=``art~1:04:00::''/&gt;
      &lt;context&gt;
        Paintings, drawings and sculpture from every period of
        &lt;head&gt;art&lt;/head&gt; during the last 350 years will be on display.
      &lt;/context&gt;
    &lt;/instance&gt;
  &lt;/lexelt&gt;
  &lt;lexelt item=``authority.n''&gt;
    &lt;instance id=``authority.40001''&gt;
      &lt;answer instance=``authority.40001'' senseid=``authority~1:14:00::''/&gt;
      &lt;context&gt;
        Not only is it allowing certain health
        &lt;head&gt;authorities&lt;/head&gt;to waste millions of pounds on
        computer systems that dont work, it also allowed the London
        ambulance service to put lives at risk with a system that had
        not been fully proven in practice.
      &lt;/context&gt;
    &lt;/instance&gt;
  &lt;/lexelt&gt;
&lt;/corpus&gt;</p>
<p>Here we have two lexelts, ``art.n'' and ``authority.n'', where ``n'' denotes
that these are noun senses of the words. We have three instances of
art with instance id's art.40001, art.40002 and art.40007
respectively, and one instance of authority with instance id
authority.40001. The first instance has two answers, while the others
have one each.</p>
<p>
</p>
<hr />
<h1><a name="detailed_description">Detailed Description</a></h1>
<p>
</p>
<h2><a name="tokenization_of_text">Tokenization of Text</a></h2>
<p>preprocess.pl accepts regular expressions from the user and
then ``tokenizes'' the text between the &lt;context&gt; &lt;/context&gt; tags. This
is done to simplify the construction of regular expressions in program
nsp2regex.pl and to achieve optimum regular expression matching in
xml2arff.pl. Following is a description of the tokenization process.</p>
<p>The text within the &lt;context&gt; &lt;/context&gt; tags is considered as one
string, the ``input'' string. This algorithm takes this input string and
creates an ``output'' string where tokens are identified and separated
from each other by a SINGLE space. Regex's provided by the user are
checked against the input string to see if a sequence of characters
starting with the first character of the string match against any of
these regex's. As soon as a we find a regular expression that does
match, this checking is halted, the matched sequence of characters is
removed from the string and appended to an ``output'' string with
exactly one space to its left and right. If none of the regex's match
against the starting characters of the input string, the first
character is considered a ``non-token''. By default this non token is
placed in angular brackets (&lt;&gt;) and then put into the output string
with one space to its left and right. This process is continued until
the input string becomes empty. This process is restarted for the next
instance.</p>
<p>For example, assume we provide the following regular expressions to
preprocess.pl:</p>
<p>&lt;head&gt;\w+&lt;/head&gt;
\w+</p>
<p>The first regular expression says that a sequence of characters
starting with ``&lt;head&gt;'', having an unbroken sequence of alphanumeric
characters and finally ending with a ``&lt;/head&gt;'' is a valid token. Also,
an unbroken sequence of alphanum characters makes a token.</p>
<p>Then, assuming that the following text occurs within the &lt;context&gt;
&lt;/context&gt; tags of an instance:</p>
<p>No, he has no &lt;head&gt;authority&lt;/head&gt; on this!</p>
<p>preprocess.pl would then convert this text to:</p>
<pre>
 No &lt;,&gt; he has no &lt;head&gt;authority&lt;/head&gt; on this &lt;!&gt;</pre>
<p>Observe that ``No'', ``he'', ``has'', ``no'', ``&lt;head&gt;authority&lt;/head&gt;'', etc
are all the tokens, while ``,'' and ``!'' arent tokens and so have been
put into angular brackets. Further observe that each token has exactly
one space to its left and right.</p>
<p>One can provide a file containing regular expressions to preprocess.pl
using the switch --token. In this file, each regular expression should
be on a line of its own and should be preceeded and succeeded with '/'
signs. Further these should be perl regular expressions.</p>
<p>Thus our regular expressions above would look like so:</p>
<p>/&lt;head&gt;\w+&lt;\/head&gt;/
/\w+/</p>
<p>We shall call the file these regular expressions lie in
``token.txt''. Then, we would run preprocess.pl on example.xml with this
token file like so:</p>
<p>preprocess.pl example.xml --token token.txt</p>
<p>
</p>
<h2><a name="various_issues_of_tokenization_wrt_preprocess_pl">Various Issues of Tokenization wrt preprocess.pl</a></h2>
<p>
</p>
<h2><a name="default_regular_expressions_">Default Regular Expressions:</a></h2>
<p>Although
tokenization is best controlled via a user specified tokenization file
designated via the --token option, there is also a default definition
of tokens that is used in the absence of a tokenization file, which
consists of the following:</p>
<p>/w+/
/[\.,;:\?!]/</p>
<p>According to this definition, a token is either a single punctuation
mark from the specified class, or it is a string of alpha-numeric
characters. Note that this default definition is generally not a good
choice for XML data since it does not treat XML tags as tokens and
will result in them ``breaking apart'' during pre-processing. For
example, given this default definition, the string :</p>
<p>&lt;head&gt;art&lt;/head&gt;</p>
<p>will be represented by preprocess.pl as</p>
<p>&lt;&lt;&gt; head &lt;&gt;&gt; art &lt;&lt;&gt; &lt;/&gt; head &lt;&gt;&gt;</p>
<p>which suggests that ``&lt;'', ``&gt;'', and ``/'' are non-tokens, while ``art'' and
``head'' are. This is unlikely to provide useful information.</p>
<p>These defaults correspond to those in NSP, which is geared towards
plain text. These are provided as a convenience, but in general we
recommend against relying upon them when processing XML data.</p>
<p>
</p>
<h2><a name="regular_expression___s___">Regular Expression /\S+/:</a></h2>
<p>Assume that the only regular expression in our token file token.txt is
/\S+/. This regular expression says that any sequence of
non-white-space characters is a token. Now, if we run the program like
so:</p>
<p>preprocess.pl example.xml --token token.txt</p>
<p>(where example.xml is the example xml file described in the previous section 
and token.txt is the file that contains the above regular
expressions /\S+/).</p>
<p>We would get all the four files, art.n.xml, art.n.count,
authority.n.xml and authority.n.count. From here on we shall show only
the ``authority'' files to save space; it is understood that the art
files are also created.</p>
<p>File authority.n.xml:</p>
<p>&lt;corpus lang='english'&gt;
&lt;lexelt item=``authority.n''&gt;
&lt;instance id=``authority.40001''&gt;
&lt;answer instance=``authority.40001'' senseid=``authority~1:14:00::''/&gt;
&lt;context&gt;</p>
<p>Not only is it allowing certain health &lt;head&gt;authorities&lt;/head&gt;to waste millions of pounds on computer systems that dont work, it also allowed the London ambulance service to put lives at risk with a system that had not been fully proven in practice. 
&lt;/context&gt;
&lt;/instance&gt;
&lt;/lexelt&gt;
&lt;/corpus&gt;</p>
<p>File authority.n.count:</p>
<p>Not only is it allowing certain health &lt;head&gt;authorities&lt;/head&gt; 
to waste millions of pounds on computer systems that dont work, 
it also allowed the London ambulance service to put lives at 
risk with a system that had not been fully proven in practice.</p>
<p>Note that every character is a part of some sequence of
non-white-space characters, and is therefore part of some token. Hence
no character is put into &lt;&gt; brackets. Also, each
non-white-space-character-sequence, that is each token, is placed in
the output with exactly one space character to its left and right.</p>
<p>
</p>
<h2><a name="regular_expression___w___">Regular Expression /\w+/:</a></h2>
<p>On the other hand if our token file token.txt were to contain the
following regex which treats every sequence of alpha numeric
characters as a token:</p>
<p>/\w+/</p>
<p>... and we were to run the program like so:</p>
<p>preprocess.pl example.xml --token token.txt</p>
<p>... then our authority files would like like so:</p>
<p>File authority.n.xml:</p>
<p>&lt;corpus lang='english'&gt;
&lt;lexelt item=``authority.n''&gt;
&lt;instance id=``authority.40001''&gt;
&lt;answer instance=``authority.40001'' senseid=``authority~1:14:00::''/&gt;
&lt;context&gt;
 Not only is it allowing certain health &lt;&lt;&gt; head &lt;&gt;&gt; authorities 
&lt;&lt;/&gt; head &lt;&gt;&gt; to waste millions of pounds on computer systems that dont 
work &lt;,&gt; it also allowed the London ambulance service to put lives at 
risk with a system that had not been fully proven in practice &lt;.&gt; 
&lt;/context&gt;
&lt;/instance&gt;
&lt;/lexelt&gt;
&lt;/corpus&gt;</p>
<p>File authority.n.count:</p>
<p>Not only is it allowing certain health &lt;&lt;&gt; head &lt;&gt;&gt; authorities 
&lt;&lt;/&gt; head &lt;&gt;&gt; to waste millions of pounds on computer systems that 
dont work &lt;,&gt; it also allowed the London ambulance service to put 
lives at risk with a system that had not been fully proven in practice &lt;.&gt;</p>
<p>Note again that since the '&lt;' and '&gt;' of the head tags are not
alpha-numeric characters they are considered as ``non-token''
characters, and are put within the &lt;&gt; tags. Further note that if there
are more than one such non-token characters one after another, they
get put into one pair of diamond brackets '&lt;' and '&gt;'. As mentioned
before, the user should include regular expressions that
preserve the tags. Thus for the above example, a regular expression
like /&lt;head&gt;\w+&lt;\/head&gt;/ would work admirably.</p>
<p>
</p>
<h2><a name="other_useful_regular_expressions_in_the_token_file_">Other Useful Regular Expressions in the Token File:</a></h2>
<p>Besides the regular expressions &lt;head&gt;\w+&lt;/head&gt; and \w+, we have
found the following regular expressions useful too.</p>
<p>/[\.,;:\?!]/  - This states that a single occurrence of one of the
                puncutation marks in the list is a token. This helps
                us specify that a puncutation mark is indeed a token
                and should not be ignored! Further, this allows us to
                create features consisting of punctuation marks using
                NSP.</p>
<p>/&amp;([^;]+;)+/  - The XML format forces us to replace certain meta
                symbols in the text by their standard formats. For
                example, if the '&lt;' symbol occurs in the text, it is
                replaced with ``&amp;lt;''. Similarly, '-' is replaced with
                ``&amp;dash;''. This regular expression recognizes these
                constructs as tokens instead of breaking them up!</p>
<p>
</p>
<h2><a name="order_of_regular_expressions_is_important_">Order of Regular Expressions Is Important:</a></h2>
<p>Recall that at every point of the ``input string'', the matching
mechanism marches down the regular expressions in the order they are
provided in the input regular expression file, and stops at the FIRST
regular expression that matches. Thus the order of the regular
expression makes a difference. For example, say our regular expression
file has the following regular expressions in this order:</p>
<p>/he/
/hear/
/\w+/</p>
<p>and our input text is ``hear me''</p>
<p>Then our output text is `` he ar me ''</p>
<p>On the other hand, if we reverse the first two regular expressions</p>
<p>/hear/
/he/
/\w+/</p>
<p>we get as output `` hear me ''</p>
<p>Thus as expected, the order of the regular expressions define how the
output will look.</p>
<p>
</p>
<h2><a name="redundant_regular_expressions_">Redundant Regular Expressions:</a></h2>
<p>Consider the following regular expressions:</p>
<p>/\S+/
/\w+/</p>
<p>As should be obvious, every token that matches the second regular
expression matches the first one too. We say that the first regular
expression ``subsumes'' the second one, and the second regular
expression is redundant. This is because the matching mechanism will
always stop at the first regular expression, and never get an
opportunity to exercise the second one. Note of course that this does
not adversely affect anything.</p>
<p>
</p>
<h2><a name="ignoring_nontokens_using_removenottoken_">Ignoring Non-Tokens using --removeNotToken:</a></h2>
<p>Recall that characters in the input string that do not match any regular  
expression as defined in token are put into angular (&lt;&gt;) brackets.   You  
may, if you wish, remove these ``non tokens'', that is not have them appear  
in the output xml and count files, by using the switch --removeNotToken.</p>
<p>Thus, for the following text:</p>
<p>No, he has no &lt;head&gt;authority&lt;/head&gt; on me!</p>
<p>and with regular expressions</p>
<p>&lt;head&gt;\w+&lt;/head&gt;
\w+</p>
<p>and if we were to run the program with the switch --removeNotToken,
preprocess.pl would convert the text into:</p>
<pre>
 No he has no &lt;head&gt;authority&lt;/head&gt; on me</pre>
<p>
</p>
<h2><a name="ignoring_nontokens_using_nontoken_">Ignoring Non-Tokens using --nontoken:</a></h2>
<p>The --nontoken option allows a user to specify a list of regular
expressions. Any strings in the input file that match this list
are removed from the file prior to tokenization.</p>
<p>It's important to note the order in which tokenization occurs.
First, those strings that match the regexes defined in nontoken
are removed. Then the strings that match the regexes defined in
token are matched. Those tokens that do not match the token
regexes are then removed. Thus, the ``order'' of precedence during
tokenization is:</p>
<p>-nontoken
-token
-removeNotToken</p>
<p>
</p>
<h2><a name="xml_output_">XML output:</a></h2>
<p>By default, for each lexical element ``word'' in the training or test
file (in the lexical sample of SENSEVAL-2), preprocess.pl creates a
file of the name ``word''.xml. For example for the file example.xml,
preprocess.pl will create files art.n.xml and authority.n.xml if it
is run as follows:</p>
<p>preprocess.pl example.xml --token token.txt</p>
<p>File art.n.xml:</p>
<p>&lt;corpus lang='english'&gt;
&lt;lexelt item=``art.n''&gt;
&lt;instance id=``art.40001''&gt;
&lt;answer instance=``art.40001'' senseid=``art~1:06:00::''/&gt;
&lt;answer instance=``art.40001'' senseid=``fine_art%1:06:00::''/&gt;
&lt;context&gt;
 &lt;head&gt;Art&lt;/head&gt; you can dance to from the creative group called Halo &lt;.&gt; 
&lt;/context&gt;
&lt;/instance&gt;
&lt;instance id=``art.40002''&gt;
&lt;answer instance=``art.40002'' senseid=``art_gallery~1:06:00::''/&gt;
&lt;context&gt;
 There &lt;'&gt; s always one to be heard somewhere during the summer in the piazza in front of the &lt;head&gt;art&lt;/head&gt; gallery and town hall or in a park &lt;.&gt; 
&lt;/context&gt;
&lt;/instance&gt;
&lt;instance id=``art.40005'' docsrc=``bnc_ckv_938''&gt;
&lt;answer instance=``art.40005'' senseid=``art~1:04:00::''/&gt;
&lt;context&gt;
 Paintings &lt;,&gt; drawings and sculpture from every period of &lt;head&gt;art&lt;/head&gt; during the last 350 years will be on display &lt;.&gt; 
&lt;/context&gt;
&lt;/instance&gt;
&lt;/lexelt&gt;
&lt;/corpus&gt;</p>
<p>File authority.n.xml:</p>
<p>&lt;corpus lang='english'&gt;
&lt;lexelt item=``authority.n''&gt;
&lt;instance id=``authority.40001''&gt;
&lt;answer instance=``authority.40001'' senseid=``authority~1:14:00::''/&gt;
&lt;context&gt;
 Not only is it allowing certain health &lt;head&gt;authorities&lt;/head&gt; to waste millions of pounds on computer systems that dont work &lt;,&gt; it also allowed the London ambulance service to put lives at risk with a system that had not been fully proven in practice &lt;.&gt; 
&lt;/context&gt;
&lt;/instance&gt;
&lt;/lexelt&gt;
&lt;/corpus&gt;</p>
<p>Observe of course that the text within the &lt;context&gt; &lt;/context&gt; region
has been tokenized as described previously according to the regular
expressions in file token.txt.</p>
<p>This default behavior can be stopped either by using the switch --xml
FILE, by which only one FILE is created, or by using the switch
--noxml, by which no xml file is created.</p>
<p>
</p>
<h2><a name="count_output_">Count output:</a></h2>
<p>Besides creating xml output, this program also creates output that can
be used directly with the program count.pl (from the Ngram Statistics
Package). After tokenizing the region within the &lt;context&gt;
&lt;/context&gt; tags of each instance, the program puts together ONLY these
pieces of text to create ``count.pl ready'' output. This is because
count.pl assumes that all tokens in the input file needs to be
``counted'' and generally we are only interested in the ``contextual''
material provided in each instance, and not the tags that occur
outside the &lt;context&gt; &lt;/context&gt; region of text.</p>
<p>By default, for each lexical element ``word'', this program creates a
file of the name word.count. For example, for the file example.xml, we
would get the files art.n.count and authority.n.count.</p>
<p>File art.n.count:</p>
<p>&lt;head&gt;Art&lt;/head&gt; you can dance to from the creative group called Halo &lt;.&gt; 
There &lt;'&gt; s always one to be heard somewhere during the summer in the piazza in front of the &lt;head&gt;art&lt;/head&gt; gallery and town hall or in a park &lt;.&gt; 
Paintings &lt;,&gt; drawings and sculpture from every period of &lt;head&gt;art&lt;/head&gt; during the last 350 years will be on display &lt;.&gt;</p>
<p>File authority.n.count:</p>
<p>Not only is it allowing certain health &lt;head&gt;authorities&lt;/head&gt; to waste millions of pounds on computer systems that dont work &lt;,&gt; it also allowed the London ambulance service to put lives at risk with a system that had not been fully proven in practice &lt;.&gt;</p>
<p>This default behavior can be stopped either by using the switch
--count FILE, by which only one FILE is created, or by using the
switch --nocount, by which no count file is created.</p>
<p>Note that the --xml/--noxml switches and the --count/--nocount
switches are independant of each other. Thus, although providing --xml
FILE or --noxml switchs produces a single xml FILE or no xml file at
all, you will still get all the count files, unless you also give the
--count FILE or --nocount switches. Similarly, providing the --count
FILE or --nocount switches does not affect the production of the xml
files.</p>
<p>
</p>
<h2><a name="information_insertion">Information Insertion</a></h2>
<p>
</p>
<h3><a name="inserting_lexelt_and_senseid_information_">Inserting lexelt and senseId Information:</a></h3>
<p>The lexelt information and the senseId information are outside the
&lt;context&gt; &lt;/context&gt; region. This program gives you the
capability to bring these pieces of information inside the context.</p>
<p>Switch --useLexelt puts the tag &lt;lexelt=WORD/&gt; within the
&lt;head&gt;&lt;/head&gt; tags, where WORD is the word in the immediately
preceding &lt;lexelt&gt; tag.</p>
<p>Switch --useSenseid puts the tag &lt;senseid=XXXXX/&gt; within the
&lt;head&gt;&lt;/head&gt; tags, where XXXXX is the number in the immediately
preceding &lt;answer&gt; tag.</p>
<p>For example, running the program like so:</p>
<p>preprocess.pl example.xml --useLexelt --useSenseid --token token.txt</p>
<p>produces this for authority.n.xml:</p>
<p>&lt;corpus lang='english'&gt;
&lt;lexelt item=``authority.n''&gt;
&lt;instance id=``authority.40001''&gt;
&lt;answer instance=``authority.40001'' senseid=``authority~1:14:00::''/&gt;
&lt;context&gt;
Not only is it allowing certain health &lt;head&gt; authorities &lt;lexelt=authority.n/&gt;&lt;senseid=authority~1:14:00::/&gt;&lt;/head&gt; to waste millions of pounds on computer systems that dont work , it also allowed the London ambulance service to put lives at risk with a system that had not been fully proven in practice . 
&lt;/context&gt;
&lt;/instance&gt;
&lt;/lexelt&gt;
&lt;/corpus&gt;</p>
<p>Note that the extra information is put inside the &lt;head&gt; &lt;/head&gt;
region. Hence the user has to provide a token file that will preserve
these &lt;head&gt; &lt;/head&gt; tags. For instance, as shown in the previous
section, if one were to rely on the default regex's, these tags would
not be preserved (the '&lt;' and '&gt;' would be considered non-token
symbols) and the lexelt and senseid information would not be included
within the tags.</p>
<p>So for example, the following regular expression file is adequate:</p>
<p>&lt;head&gt;\w+&lt;/head&gt;
\w+</p>
<p>
</p>
<h3><a name="inserting_sentenceboundary_tags">Inserting Sentence-Boundary Tags</a></h3>
<p>The english lexical sample data available from SENSEVAL-2 is such that
each sentence within the &lt;context&gt; &lt;/context&gt; tags is on a line of its
own. This human-detected sentence boundary information is usually lost
in preprocess.pl, but can be preserved using the switch
--putSentenceTags. This puts each line within &lt;s&gt; and &lt;/s&gt;
tags. Assuming that each sentence was originally on a line of its own,
then &lt;s&gt; marks the start of a sentence and &lt;/s&gt; marks its end. Note
that no sentence boundary detection is done: if the end of line
character (\n) does not match the end of a sentence, then the &lt;s&gt; &lt;/s&gt;
tags will not be indicative of a sentence boundary either.</p>
<p>For example, assume the following is our source xml file, source.xml:</p>
<p>&lt;corpus lang='english'&gt;
&lt;lexelt item=``word''&gt;
&lt;instance id=``word.1''&gt;
&lt;answer instance=``word.1'' senseid=``1''/&gt;
&lt;context&gt;
This is the first line
This is the second line
This is the last line for &lt;head&gt;word&lt;/head&gt;
&lt;/context&gt;
&lt;/instance&gt;
&lt;/lexelt&gt;
&lt;/corpus&gt;</p>
<p>Further assume our token file is this:</p>
<p>/&lt;head&gt;\w+&lt;/head&gt;/
/&lt;s&gt;/
/&lt;\/s&gt;/
/\w+/</p>
<p>Running preprocess.pl like so:</p>
<p>preprocess.pl --token token.txt source.xml</p>
<p>Produces the following word.xml file:</p>
<p>&lt;corpus lang='english'&gt;
&lt;lexelt item=``word''&gt;
&lt;instance id=``word.1''&gt;
&lt;answer instance=``word.1'' senseid=``1''/&gt;
&lt;context&gt;
 This is the first line This is the second line This is the last line for &lt;head&gt;word&lt;/head&gt; 
&lt;/context&gt;
&lt;/instance&gt;
&lt;/lexelt&gt;
&lt;/corpus&gt;</p>
<p>and the following word.count file:</p>
<pre>
 This is the first line This is the second line This is the last line for &lt;head&gt;word&lt;/head&gt;</pre>
<p>However, running preprocess.pl like so:</p>
<p>preprocess.pl --token token.txt --putSentenceTags source.xml</p>
<p>Produces the following word.xml file:</p>
<p>&lt;corpus lang='english'&gt;
&lt;lexelt item=``word''&gt;
&lt;instance id=``word.1''&gt;
&lt;answer instance=``word.1'' senseid=``1''/&gt;
&lt;context&gt;
 &lt;s&gt; This is the first line &lt;/s&gt; &lt;s&gt; This is the second line &lt;/s&gt; &lt;s&gt; This is the last line for &lt;head&gt;word&lt;/head&gt; &lt;/s&gt; 
&lt;/context&gt;
&lt;/instance&gt;
&lt;/lexelt&gt;
&lt;/corpus&gt;</p>
<p>and the following word.count file:</p>
<pre>
 &lt;s&gt; This is the first line &lt;/s&gt; &lt;s&gt; This is the second line &lt;/s&gt; &lt;s&gt; This is the last line for &lt;head&gt;word&lt;/head&gt; &lt;/s&gt;</pre>
<p>Note that the &lt;s&gt; and &lt;/s&gt; tags are placed into the data BEFORE the
tokenization process. Hence a token regular expression that preserves
these tags is required! The token file shown above is adequate for
this.</p>
<p>
</p>
<h2><a name="splitting_input_lexical_files">Splitting Input Lexical Files</a></h2>
<p>Besides splitting the lexical elements into separate files,
preprocess.pl also allows you to split the instances of a single
lexical element into separate ``training'' and ``test'' files.</p>
<p>If one has a corpus of sense-tagged text, it is
often desirable to divide that sense tagged text into training and
test portions in order to develop or tune  a methodology. This is the
intention of the --split option.</p>
<p>The --split option of preprocess.pl allows you to specify an integer
N... the instances of each lexical element in the input XML SOURCE
file are split into two files approximately in the ratio N:(100-N).</p>
<p>If an output XML file ``foo'' is specified through the switch --xml then
two files, foo-training.xml and foo-test.xml are created.</p>
<p>If an output count file ``foo'' is specified through the switch --count
then two files, foo-training.count and foo-test.count are created.</p>
<p>Creation of Xml and count output files can be suppressed by using the
--noxml and --nocount switches respectively.</p>
<p>If neither --noxml nor --xml switches are used, then files of the type
word-training.xml, word-test.xml are created.</p>
<p>If neither --nocount nor --count switches are used, then files of the
type word-training.count, word-test.count are created.</p>
<p>The instances are shuffled before being put into training and test
files. Perl automatically seeds the randomizing process... but you can
specify your own seed using the switch --seed.</p>
<p>
</p>
<hr />
<h1><a name="author">AUTHOR</a></h1>
<pre>
 Satanjeev Banerjee, Carnegie Mellon University, Pittsburgh.
 Ted Pedersen, University of Minnesota, Duluth.</pre>
<p>
</p>
<hr />
<h1><a name="copyright">COPYRIGHT</a></h1>
<p>Copyright (c) 2001-2005,</p>
<pre>
 Satanjeev Banerjee, Carnegie Mellon University, Pittsburgh.
 satanjeev@cmu.edu</pre>
<pre>
 Ted Pedersen, University of Minnesota, Duluth.
 tpederse@umn.edu</pre>
<p>This program is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free Software
Foundation; either version 2 of the License, or (at your option) any later
version.</p>
<p>This program is distributed in the hope that it will be useful, but WITHOUT
ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.</p>
<p>You should have received a copy of the GNU General Public License along with
this program; if not, write to</p>
<p>The Free Software Foundation, Inc.,
59 Temple Place - Suite 330,
Boston, MA  02111-1307, USA.</p>

</body>

</html>
